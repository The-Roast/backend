{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from scholarly import scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = arxiv.Search(\n",
    "    query=\"artificial intelligence\",\n",
    "    sort_by=arxiv.SortCriterion.SubmittedDate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "papers: list[arxiv.Result] = []\n",
    "for result in search.results():\n",
    "    if c >= 100: break\n",
    "    if not result.journal_ref: continue\n",
    "    citations = []\n",
    "    for author in result.authors:\n",
    "        try:\n",
    "            info = next(scholarly.search_author(author.name))\n",
    "            citations.append(info.get(\"citedby\", 0))\n",
    "        except StopIteration as e:\n",
    "            citations.append(0)\n",
    "    papers.append((result, sum(citations) / len(citations)))\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spapers = sorted(papers, reverse=True, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fspapers = [paper for (paper, citation) in spapers if \"cs\" in str(paper.primary_category) or \"ML\" in str(paper.primary_category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[arxiv.Result(entry_id='http://arxiv.org/abs/2309.02009v1', updated=datetime.datetime(2023, 9, 5, 7, 47, 8, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 9, 5, 7, 47, 8, tzinfo=datetime.timezone.utc), title='Belief revision and incongruity: is it a joke?', authors=[arxiv.Result.Author('Florence Dupin de Saint Cyr - Bannay'), arxiv.Result.Author('Henri Prade')], summary='Incongruity often makes people laugh. You have to be smart to say stupid\\nthings. It requires to be even smarter for understanding them. This paper is a\\nshameless attempt to formalize this intelligent behavior in the case of an\\nagent listening to a joke. All this is a matter of revision of beliefs,\\nsurprise and violation of norms.', comment='A special paper on/in humor/honor for/of Philippe Besnard', journal_ref='Journal of Applied Non-Classical Logics, In press, Special issue\\n  in honour of Philippe Besnard, pp.1-28', doi='10.1080/11663081.2023.2244379', primary_category='cs.AI', categories=['cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1080/11663081.2023.2244379', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2309.02009v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2309.02009v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2309.04977v1', updated=datetime.datetime(2023, 9, 10, 9, 46, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 9, 10, 9, 46, 38, tzinfo=datetime.timezone.utc), title='RGAT: A Deeper Look into Syntactic Dependency Information for Coreference Resolution', authors=[arxiv.Result.Author('Yuan Meng'), arxiv.Result.Author('Xuhao Pan'), arxiv.Result.Author('Jun Chang'), arxiv.Result.Author('Yue Wang')], summary='Although syntactic information is beneficial for many NLP tasks, combining it\\nwith contextual information between words to solve the coreference resolution\\nproblem needs to be further explored. In this paper, we propose an end-to-end\\nparser that combines pre-trained BERT with a Syntactic Relation Graph Attention\\nNetwork (RGAT) to take a deeper look into the role of syntactic dependency\\ninformation for the coreference resolution task. In particular, the RGAT model\\nis first proposed, then used to understand the syntactic dependency graph and\\nlearn better task-specific syntactic embeddings. An integrated architecture\\nincorporating BERT embeddings and syntactic embeddings is constructed to\\ngenerate blending representations for the downstream task. Our experiments on a\\npublic Gendered Ambiguous Pronouns (GAP) dataset show that with the supervision\\nlearning of the syntactic dependency graph and without fine-tuning the entire\\nBERT, we increased the F1-score of the previous best model (RGCN-with-BERT)\\nfrom 80.3% to 82.5%, compared to the F1-score by single BERT embeddings from\\n78.5% to 82.5%. Experimental results on another public dataset - OntoNotes 5.0\\ndemonstrate that the performance of the model is also improved by incorporating\\nsyntactic dependency information learned from RGAT.', comment='8 pages, 5 figures', journal_ref='2023 International Joint Conference on Neural Networks (IJCNN)', doi='10.1109/IJCNN54540.2023.10191577', primary_category='cs.CL', categories=['cs.CL', 'cs.AI', '14J60 (Primary) 14F05, 14J26 (Secondary)'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/IJCNN54540.2023.10191577', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2309.04977v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2309.04977v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2308.14395v1', updated=datetime.datetime(2023, 8, 28, 8, 20, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 8, 28, 8, 20, 30, tzinfo=datetime.timezone.utc), title='UMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization', authors=[arxiv.Result.Author('Rui Zhang'), arxiv.Result.Author('Hongxia Wang'), arxiv.Result.Author('Mingshan Du'), arxiv.Result.Author('Hanqing Liu'), arxiv.Result.Author('Yang Zhou'), arxiv.Result.Author('Qiang Zeng')], summary='The emergence of artificial intelligence-generated content (AIGC) has raised\\nconcerns about the authenticity of multimedia content in various fields.\\nHowever, existing research for forgery content detection has focused mainly on\\nbinary classification tasks of complete videos, which has limited applicability\\nin industrial settings. To address this gap, we propose UMMAFormer, a novel\\nuniversal transformer framework for temporal forgery localization (TFL) that\\npredicts forgery segments with multimodal adaptation. Our approach introduces a\\nTemporal Feature Abnormal Attention (TFAA) module based on temporal feature\\nreconstruction to enhance the detection of temporal differences. We also design\\na Parallel Cross-Attention Feature Pyramid Network (PCA-FPN) to optimize the\\nFeature Pyramid Network (FPN) for subtle feature enhancement. To evaluate the\\nproposed method, we contribute a novel Temporal Video Inpainting Localization\\n(TVIL) dataset specifically tailored for video inpainting scenes. Our\\nexperiments show that our approach achieves state-of-the-art performance on\\nbenchmark datasets, including Lav-DF, TVIL, and Psynd, significantly\\noutperforming previous methods. The code and data are available at\\nhttps://github.com/ymhzyj/UMMAFormer/.', comment='11 pages, 8 figures, 66 references. This paper has been accepted for\\n  ACM MM 2023', journal_ref=\"Proceedings of the 31st ACM International Conference on Multimedia\\n  (MM '23), October 29-November 3, 2023\", doi='10.1145/3581783.3613767', primary_category='cs.MM', categories=['cs.MM', 'cs.CV', '68T45', 'I.4'], links=[arxiv.Result.Link('http://dx.doi.org/10.1145/3581783.3613767', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2308.14395v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2308.14395v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2308.09691v1', updated=datetime.datetime(2023, 8, 18, 17, 38, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 8, 18, 17, 38, tzinfo=datetime.timezone.utc), title='Reduced Order Modeling of a MOOSE-based Advanced Manufacturing Model with Operator Learning', authors=[arxiv.Result.Author('Mahmoud Yaseen'), arxiv.Result.Author('Dewen Yushu'), arxiv.Result.Author('Peter German'), arxiv.Result.Author('Xu Wu')], summary='Advanced Manufacturing (AM) has gained significant interest in the nuclear\\ncommunity for its potential application on nuclear materials. One challenge is\\nto obtain desired material properties via controlling the manufacturing process\\nduring runtime. Intelligent AM based on deep reinforcement learning (DRL)\\nrelies on an automated process-level control mechanism to generate optimal\\ndesign variables and adaptive system settings for improved end-product\\nproperties. A high-fidelity thermo-mechanical model for direct energy\\ndeposition has recently been developed within the MOOSE framework at the Idaho\\nNational Laboratory (INL). The goal of this work is to develop an accurate and\\nfast-running reduced order model (ROM) for this MOOSE-based AM model that can\\nbe used in a DRL-based process control and optimization method. Operator\\nlearning (OL)-based methods will be employed due to their capability to learn a\\nfamily of differential equations, in this work, produced by changing process\\nvariables in the Gaussian point heat source for the laser. We will develop\\nOL-based ROM using Fourier neural operator, and perform a benchmark comparison\\nof its performance with a conventional deep neural network-based ROM.', comment='10 Pages, 7 Figures, 2 Tables. arXiv admin note: text overlap with\\n  arXiv:2308.02462', journal_ref='In Proceedings of the 2023 International Conference on Mathematics\\n  and Computational Methods Applied to Nuclear Science and Engineering (M&C\\n  2023)', doi=None, primary_category='stat.ML', categories=['stat.ML', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2308.09691v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2308.09691v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2308.14009v1', updated=datetime.datetime(2023, 8, 27, 5, 45, 54, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 8, 27, 5, 45, 54, tzinfo=datetime.timezone.utc), title='Towards Fast and Accurate Image-Text Retrieval with Self-Supervised Fine-Grained Alignment', authors=[arxiv.Result.Author('Jiamin Zhuang'), arxiv.Result.Author('Jing Yu'), arxiv.Result.Author('Yang Ding'), arxiv.Result.Author('Xiangyan Qu'), arxiv.Result.Author('Yue Hu')], summary='Image-text retrieval requires the system to bridge the heterogenous gap\\nbetween vision and language for accurate retrieval while keeping the network\\nlightweight-enough for efficient retrieval. Existing trade-off solutions mainly\\nstudy from the view of incorporating cross-modal interactions with the\\nindependent-embedding framework or leveraging stronger pretrained encoders,\\nwhich still demand time-consuming similarity measurement or heavyweight model\\nstructure in the retrieval stage. In this work, we propose an image-text\\nalignment module SelfAlign on top of the independent-embedding framework, which\\nimproves the retrieval accuracy while maintains the retrieval efficiency\\nwithout extra supervision. SelfAlign contains two collaborative sub-modules\\nthat force image-text alignment at both concept level and context level by\\nself-supervised contrastive learning. It does not require cross-modal embedding\\ninteractions during training while maintaining independent image and text\\nencoders during retrieval. With comparable time cost, SelfAlign consistently\\nboosts the accuracy of state-of-the-art non-pretraining independent-embedding\\nmodels respectively by 9.1%, 4.2% and 6.6% in terms of R@sum score on\\nFlickr30K, MSCOCO 1K and MS-COCO 5K datasets. The retrieval accuracy also\\noutperforms most existing interactive-embedding models with orders of magnitude\\ndecrease in retrieval time. The source code is available at:\\nhttps://github.com/Zjamie813/SelfAlign.', comment='Accepted in IEEE Transactions on Multimedia (TMM)', journal_ref='IEEE Transactions on Multimedia ( Early Access ), 29 May 2023', doi='10.1109/TMM.2023.3280734', primary_category='cs.CV', categories=['cs.CV', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.1109/TMM.2023.3280734', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2308.14009v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2308.14009v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2308.13165v1', updated=datetime.datetime(2023, 8, 25, 4, 6, 30, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 8, 25, 4, 6, 30, tzinfo=datetime.timezone.utc), title='Dual Compensation Residual Networks for Class Imbalanced Learning', authors=[arxiv.Result.Author('Ruibing Hou'), arxiv.Result.Author('Hong Chang'), arxiv.Result.Author('Bingpeng Ma'), arxiv.Result.Author('Shiguang Shan'), arxiv.Result.Author('Xilin Chen')], summary='Learning generalizable representation and classifier for class-imbalanced\\ndata is challenging for data-driven deep models. Most studies attempt to\\nre-balance the data distribution, which is prone to overfitting on tail classes\\nand underfitting on head classes. In this work, we propose Dual Compensation\\nResidual Networks to better fit both tail and head classes. Firstly, we propose\\ndual Feature Compensation Module (FCM) and Logit Compensation Module (LCM) to\\nalleviate the overfitting issue. The design of these two modules is based on\\nthe observation: an important factor causing overfitting is that there is\\nsevere feature drift between training and test data on tail classes. In\\ndetails, the test features of a tail category tend to drift towards feature\\ncloud of multiple similar head categories. So FCM estimates a multi-mode\\nfeature drift direction for each tail category and compensate for it.\\nFurthermore, LCM translates the deterministic feature drift vector estimated by\\nFCM along intra-class variations, so as to cover a larger effective\\ncompensation space, thereby better fitting the test features. Secondly, we\\npropose a Residual Balanced Multi-Proxies Classifier (RBMC) to alleviate the\\nunder-fitting issue. Motivated by the observation that re-balancing strategy\\nhinders the classifier from learning sufficient head knowledge and eventually\\ncauses underfitting, RBMC utilizes uniform learning with a residual path to\\nfacilitate classifier learning. Comprehensive experiments on Long-tailed and\\nClass-Incremental benchmarks validate the efficacy of our method.', comment='20 pages', journal_ref='IEEE Transactions on Pattern Analysis and Machine Intelligence\\n  (TPAMI,2023)', doi=None, primary_category='cs.CV', categories=['cs.CV'], links=[arxiv.Result.Link('http://arxiv.org/abs/2308.13165v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2308.13165v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2309.01901v1', updated=datetime.datetime(2023, 9, 5, 2, 16, 45, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 9, 5, 2, 16, 45, tzinfo=datetime.timezone.utc), title='Towards General and Efficient Online Tuning for Spark', authors=[arxiv.Result.Author('Yang Li'), arxiv.Result.Author('Huaijun Jiang'), arxiv.Result.Author('Yu Shen'), arxiv.Result.Author('Yide Fang'), arxiv.Result.Author('Xiaofeng Yang'), arxiv.Result.Author('Danqing Huang'), arxiv.Result.Author('Xinyi Zhang'), arxiv.Result.Author('Wentao Zhang'), arxiv.Result.Author('Ce Zhang'), arxiv.Result.Author('Peng Chen'), arxiv.Result.Author('Bin Cui')], summary='The distributed data analytic system -- Spark is a common choice for\\nprocessing massive volumes of heterogeneous data, while it is challenging to\\ntune its parameters to achieve high performance. Recent studies try to employ\\nauto-tuning techniques to solve this problem but suffer from three issues:\\nlimited functionality, high overhead, and inefficient search.\\n  In this paper, we present a general and efficient Spark tuning framework that\\ncan deal with the three issues simultaneously. First, we introduce a\\ngeneralized tuning formulation, which can support multiple tuning goals and\\nconstraints conveniently, and a Bayesian optimization (BO) based solution to\\nsolve this generalized optimization problem. Second, to avoid high overhead\\nfrom additional offline evaluations in existing methods, we propose to tune\\nparameters along with the actual periodic executions of each job (i.e., online\\nevaluations). To ensure safety during online job executions, we design a safe\\nconfiguration acquisition method that models the safe region. Finally, three\\ninnovative techniques are leveraged to further accelerate the search process:\\nadaptive sub-space generation, approximate gradient descent, and meta-learning\\nmethod.\\n  We have implemented this framework as an independent cloud service, and\\napplied it to the data platform in Tencent. The empirical results on both\\npublic benchmarks and large-scale production tasks demonstrate its superiority\\nin terms of practicality, generality, and efficiency. Notably, this service\\nsaves an average of 57.00% memory cost and 34.93% CPU cost on 25K in-production\\ntasks within 20 iterations, respectively.', comment=None, journal_ref='Proceedings of the VLDB Endowment 2023', doi='10.14778/3611540.3611548', primary_category='cs.DC', categories=['cs.DC', 'cs.AI'], links=[arxiv.Result.Link('http://dx.doi.org/10.14778/3611540.3611548', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2309.01901v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2309.01901v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2308.09976v1', updated=datetime.datetime(2023, 8, 19, 10, 43, 11, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 8, 19, 10, 43, 11, tzinfo=datetime.timezone.utc), title='Explicit Time Embedding Based Cascade Attention Network for Information Popularity Prediction', authors=[arxiv.Result.Author('Xigang Sun'), arxiv.Result.Author('Jingya Zhou'), arxiv.Result.Author('Ling Liu'), arxiv.Result.Author('Wenqi Wei')], summary='Predicting information cascade popularity is a fundamental problem in social\\nnetworks. Capturing temporal attributes and cascade role information (e.g.,\\ncascade graphs and cascade sequences) is necessary for understanding the\\ninformation cascade. Current methods rarely focus on unifying this information\\nfor popularity predictions, which prevents them from effectively modeling the\\nfull properties of cascades to achieve satisfactory prediction performances. In\\nthis paper, we propose an explicit Time embedding based Cascade Attention\\nNetwork (TCAN) as a novel popularity prediction architecture for large-scale\\ninformation networks. TCAN integrates temporal attributes (i.e., periodicity,\\nlinearity, and non-linear scaling) into node features via a general time\\nembedding approach (TE), and then employs a cascade graph attention encoder\\n(CGAT) and a cascade sequence attention encoder (CSAT) to fully learn the\\nrepresentation of cascade graphs and cascade sequences. We use two real-world\\ndatasets (i.e., Weibo and APS) with tens of thousands of cascade samples to\\nvalidate our methods. Experimental results show that TCAN obtains mean\\nlogarithm squared errors of 2.007 and 1.201 and running times of 1.76 hours and\\n0.15 hours on both datasets, respectively. Furthermore, TCAN outperforms other\\nrepresentative baselines by 10.4%, 3.8%, and 10.4% in terms of MSLE, MAE, and\\nR-squared on average while maintaining good interpretability.', comment=None, journal_ref='Inf. Process. Manag. 60(3): 103278 (2023)', doi='10.1016/j.ipm.2023.103278', primary_category='cs.SI', categories=['cs.SI', 'cs.AI', 'cs.IR'], links=[arxiv.Result.Link('http://dx.doi.org/10.1016/j.ipm.2023.103278', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2308.09976v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2308.09976v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2308.10207v1', updated=datetime.datetime(2023, 8, 20, 9, 15, 21, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 8, 20, 9, 15, 21, tzinfo=datetime.timezone.utc), title='Affective Digital Twins for Digital Human: Bridging the Gap in Human-Machine Affective Interaction', authors=[arxiv.Result.Author('Feng Lu'), arxiv.Result.Author('Bo Liu')], summary='In recent years, metaverse and digital humans have become important research\\nand industry areas of focus. However, existing digital humans still lack\\nrealistic affective traits, making emotional interaction with humans difficult.\\nGrounded in the developments of artificial intelligence, human-computer\\ninteraction, virtual reality, and affective computing, this paper proposes the\\nconcept and technical framework of \"Affective Digital Twins for Digital Human\"\\nbased on the philosophy of digital twin technology. The paper discusses several\\nkey technical issues including affective modeling, affective perception,\\naffective encoding, and affective expression. Based on this, the paper conducts\\na preliminary imagination of the future application prospects of affective\\ndigital twins for digital human, while considering potential problems that may\\nneed to be addressed.', comment='7 pages, in Chinese language, 6 figures', journal_ref='Journal of Chinese Association for Artificial Intelligence, 13(3),\\n  11-16 (2023)', doi=None, primary_category='cs.HC', categories=['cs.HC'], links=[arxiv.Result.Link('http://arxiv.org/abs/2308.10207v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2308.10207v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2309.00966v1', updated=datetime.datetime(2023, 9, 2, 15, 20, 36, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 9, 2, 15, 20, 36, tzinfo=datetime.timezone.utc), title='Compositional Diffusion-Based Continuous Constraint Solvers', authors=[arxiv.Result.Author('Zhutian Yang'), arxiv.Result.Author('Jiayuan Mao'), arxiv.Result.Author('Yilun Du'), arxiv.Result.Author('Jiajun Wu'), arxiv.Result.Author('Joshua B. Tenenbaum'), arxiv.Result.Author('Tomás Lozano-Pérez'), arxiv.Result.Author('Leslie Pack Kaelbling')], summary='This paper introduces an approach for learning to solve continuous constraint\\nsatisfaction problems (CCSP) in robotic reasoning and planning. Previous\\nmethods primarily rely on hand-engineering or learning generators for specific\\nconstraint types and then rejecting the value assignments when other\\nconstraints are violated. By contrast, our model, the compositional diffusion\\ncontinuous constraint solver (Diffusion-CCSP) derives global solutions to CCSPs\\nby representing them as factor graphs and combining the energies of diffusion\\nmodels trained to sample for individual constraint types. Diffusion-CCSP\\nexhibits strong generalization to novel combinations of known constraints, and\\nit can be integrated into a task and motion planner to devise long-horizon\\nplans that include actions with both discrete and continuous parameters.\\nProject site: https://diffusion-ccsp.github.io/', comment=None, journal_ref='Proceedings of CoRL 2023', doi=None, primary_category='cs.RO', categories=['cs.RO', 'cs.AI', 'cs.LG'], links=[arxiv.Result.Link('http://arxiv.org/abs/2309.00966v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2309.00966v1', title='pdf', rel='related', content_type=None)]),\n",
       " arxiv.Result(entry_id='http://arxiv.org/abs/2309.07508v1', updated=datetime.datetime(2023, 9, 14, 8, 20, 27, tzinfo=datetime.timezone.utc), published=datetime.datetime(2023, 9, 14, 8, 20, 27, tzinfo=datetime.timezone.utc), title='An Open RAN Framework for the Dynamic Control of 5G Service Level Agreements', authors=[arxiv.Result.Author('Eugenio Moro'), arxiv.Result.Author('Michele Polese'), arxiv.Result.Author('Antonio Capone'), arxiv.Result.Author('Tommaso Melodia')], summary='The heterogeneity of use cases that next-generation wireless systems need to\\nsupport calls for flexible and programmable networks that can autonomously\\nadapt to the application requirements. Specifically, traffic flows that support\\ncritical applications (e.g., vehicular control or safety communications) often\\ncome with a requirement in terms of guaranteed performance. At the same time,\\nothers are more elastic and can adapt to the resources made available by the\\nnetwork (e.g., video streaming). To this end, the Open Radio Access Network\\n(RAN) paradigm is seen as an enabler of dynamic control and adaptation of the\\nprotocol stack of 3rd Generation Partnership Project (3GPP) networks in the 5th\\nGeneration (5G) and beyond. Through its embodiment in the O-RAN alliance\\nspecifications, it introduces the Ran Intelligent Controllers (RICs), which\\nenable closed-loop control, leveraging a rich set of RAN Key Performance\\nMeasurements (KPMs) to build a representation of the network and enforcing\\ndynamic control through the configuration of 3GPP-defined stack parameters. In\\nthis paper, we leverage the Open RAN closed-loop control capabilities to\\ndesign, implement, and evaluate multiple data-driven and dynamic Service Level\\nAgreement (SLA) enforcement policies, capable of adapting the RAN\\nsemi-persistent scheduling patterns to match users requirements. To do so, we\\nimplement semi-persistent scheduling capabilities in the OpenAirInterface (OAI)\\n5G stack, as well as an easily extensible and customizable version of the Open\\nRAN E2 interface that connects the OAI base stations to the near-real-time RIC.\\nWe deploy and test our framework on Colosseum, a large-scale\\nhardware-in-the-loop channel emulator. Results confirm the effectiveness of the\\nproposed Open RAN-based solution in managing SLA in near-real-time.', comment=None, journal_ref='2023 IEEE Conference on Network Function Virtualization and\\n  Software Defined Networks (NFV-SDN 23)', doi=None, primary_category='cs.NI', categories=['cs.NI'], links=[arxiv.Result.Link('http://arxiv.org/abs/2309.07508v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2309.07508v1', title='pdf', rel='related', content_type=None)])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fspapers[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
